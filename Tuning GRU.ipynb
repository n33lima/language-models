{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Activation, Dense, Dropout, Input, Embedding,Flatten\n",
    "from keras import Model\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.backend import clear_session\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer, HashingVectorizer\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.backend import clear_session\n",
    "\n",
    "clear_session()\n",
    "# dataset\n",
    "df=pd.read_csv('/Users/joe/Desktop/language-models-sprint1/data/train.csv')\n",
    "\n",
    "\n",
    "#Split the data into train and test sets\n",
    "X = df['text']\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "#Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "\n",
    "# apply tokonizer\n",
    "tokenizer = Tokenizer(lower=True,num_words=40000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_tok = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# padding zerros to insure the sequence of words in a text is equal\n",
    "# the tweet is 280 long, so I set the padding to \n",
    "padding_length=280\n",
    "X_train_tok=pad_sequences(X_train_tok,padding='post',maxlen=padding_length)\n",
    "X_test=pad_sequences(X_test,padding='post',maxlen=padding_length)\n",
    "\n",
    "\n",
    "#early stopping \n",
    "early_stop=EarlyStopping(monitor='loss', patience=1, mode='min', verbose=1)\n",
    "\n",
    "\n",
    "#building model method\n",
    "\n",
    "#building model method\n",
    "def build_model(learning_rate,vocab_size,padding_length,\n",
    "                neurons_of_GRU_layer,neurons_of_second_layer,GRU_activation,second_layer_activation,\n",
    "                output_layer_activation\n",
    "                ,optimizer,recurrent_dropout,dropout,embedding_dim\n",
    "                ):\n",
    "    inputs= Input(name='inputs',shape=[padding_length])\n",
    "    layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=padding_length)(inputs)\n",
    "    layer = Bidirectional(GRU(neurons_of_GRU_layer, activation=GRU_activation,recurrent_dropout=recurrent_dropout))(layer)\n",
    "    layer = Dense(85, activation=second_layer_activation)(layer)\n",
    "    layer = Dropout(dropout)(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(1, activation='sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# the parameter grid\n",
    "tuning_parameters = dict(epochs=[20,40,60],\n",
    "                  batch_size=[200,400,800],\n",
    "                  vocab_size=[25077], \n",
    "                  embedding_dim=[20,40],\n",
    "                  learning_rate = [0.0001,0.0003,0.0007],\n",
    "                  neurons_of_GRU_layer=[20,30],\n",
    "                  neurons_of_second_layer=[45,85],\n",
    "                  padding_length=[280],\n",
    "                  GRU_activation=['sigmoid','relu','tanh'],\n",
    "                  second_layer_activation=['sigmoid','relu','tanh'],\n",
    "                  output_layer_activation=['sigmoid','relu'],\n",
    "                  optimizer=['RMSprop','Adam'],\n",
    "                  recurrent_dropout=[0.15,0.20,0.25],\n",
    "                  dropout=[0.05,0.10]\n",
    "         \n",
    "               \n",
    "                )\n",
    "\n",
    "# Running the RandomizedSearchCV\n",
    "\n",
    "model = KerasClassifier(build_fn=build_model,verbose=1)\n",
    "tuning = RandomizedSearchCV(estimator=model, param_distributions=tuning_parameters,\n",
    "                              cv=10, verbose=1, n_jobs=-1,return_train_score=True)\n",
    "\n",
    "tuning_result = tuning.fit(X_train_tok, y_train,callbacks=[early_stop],validation_split=0.05)\n",
    "test_accuracy = tuning.score(X_test, y_test)\n",
    "training_accuracy=tuning.score(X_train_tok,y_train)\n",
    "\n",
    "print('best score')\n",
    "print(tuning_result.best_score_)\n",
    "print('best parameters')\n",
    "print(tuning_result.best_params_)\n",
    "print('best test accuracy')\n",
    "print(test_accuracy)\n",
    "print('best Training accuracy')\n",
    "print(training_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
