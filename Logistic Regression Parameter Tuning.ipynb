{
  "cells":[
    {
      "cell_type":"code",
      "source":[
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import datetime,time\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression,LogisticRegressionCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.svm import SVC\n",
        "import math\n",
        "import statistics\n",
        "from sklearn.linear_model import SGDClassifier"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "\n",
        "# 4 ML models without Cross-Validation\n",
        "df=pd.read_csv('\/data\/workspace_files\/train.csv')\n",
        "X = df['text']\n",
        "y = df['target']\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
        "\n",
        "# pipe for each model\n",
        "pipe1 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression()),])\n",
        "\n",
        "pipe2 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression(solver='newton-cg')),])\n",
        "\n",
        "pipe3 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression(solver='liblinear')),])\n",
        "\n",
        "pipe4 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression(solver='sag')),])\n",
        "\n",
        "pipe5 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression(solver='saga')),])\n",
        "\n",
        "pipe6 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression(penalty='l1',solver='liblinear')),])\n",
        "\n",
        "pipe7 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression(penalty='l1',solver='saga')),])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# list of thr models as pipes\n",
        "list_of_models = {'Logistic Regression with LBFGS solver and L2 penalty':pipe1,\n",
        "                  'Logistic Regression with Newton-CG solver and L2 penalty':pipe2, \n",
        "                  'Logistic Regression with lib linear solver and L2 penalty':pipe3, \n",
        "                  'Logistic Regression with SAG solver and L2 penalty':pipe4, \n",
        "                  'Logistic Regression with SAGA solver and L2 penalty':pipe5, \n",
        "                  'Logistic Regression with lib linear solver and L1 penalty':pipe6, \n",
        "                  'Logistic Regression with SAG solver and L1 penalty':pipe7 }\n",
        "\n",
        "\n",
        "def get_Model_results(models,X_train,y_train,X_test,y_test):\n",
        "  \n",
        "    for model_name, model in list_of_models.items():\n",
        "        \n",
        "        start_time =  time.time()%60\n",
        "        text_clf = model.fit(X_train, y_train)\n",
        "        end_time= time.time()%60\n",
        "        \n",
        "        if(end_time>start_time):\n",
        "            time_to_build_the_model='%.2f' %(end_time-start_time)\n",
        "        else:\n",
        "            time_to_build_the_model='%.2f' %(start_time-end_time)\n",
        "                \n",
        "        \n",
        "        start_time = time.time()%60\n",
        "        predicted = text_clf.predict(X_test)\n",
        "        end_time= time.time()%60\n",
        "        \n",
        "        if(end_time>start_time):\n",
        "            time_to_predict='%.2f' %(end_time-start_time)\n",
        "        else:\n",
        "            time_to_predict='%.2f' %(start_time-end_time)\n",
        "                \n",
        "        \n",
        "        print('\\n................................................\\nModel {}'.format(model_name))\n",
        "        print(confusion_matrix(y_test,predicted))\n",
        "        print(classification_report(y_test,predicted))\n",
        "        print('\\nAccuracy on Training:\\n{00:.2f}'.format(text_clf.score(X_train,y_train)*100))\n",
        "    \n",
        "        print('\\nAccuracy on Testing:\\n{00:.2f}'.format(accuracy_score(y_test,predicted)*100))\n",
        "       \n",
        "        print('\\nTime taken to build the model is {} Seconds'.format(time_to_build_the_model))\n",
        "        print('\\nTime taken for prediction is {} Seconds'.format(time_to_predict))\n",
        "\n",
        "\n",
        "\n",
        "get_Model_results(list_of_models,X_train,y_train,X_test,y_test)\n"
      ],
      "execution_count":0,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\n",
            "................................................\n",
            "Model Logistic Regression with LBFGS solver and L2 penalty\n",
            "[[1157  161]\n",
            " [ 280  686]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84      1318\n",
            "           1       0.81      0.71      0.76       966\n",
            "\n",
            "    accuracy                           0.81      2284\n",
            "   macro avg       0.81      0.79      0.80      2284\n",
            "weighted avg       0.81      0.81      0.80      2284\n",
            "\n",
            "\n",
            "Accuracy on Training:\n",
            "88.87\n",
            "\n",
            "Accuracy on Testing:\n",
            "80.69\n",
            "\n",
            "Time taken to build the model is 1.07 Seconds\n",
            "\n",
            "Time taken for prediction is 0.19 Seconds\n",
            "\n",
            "................................................\n",
            "Model Logistic Regression with Newton-CG solver and L2 penalty\n",
            "[[1157  161]\n",
            " [ 280  686]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84      1318\n",
            "           1       0.81      0.71      0.76       966\n",
            "\n",
            "    accuracy                           0.81      2284\n",
            "   macro avg       0.81      0.79      0.80      2284\n",
            "weighted avg       0.81      0.81      0.80      2284\n",
            "\n",
            "\n",
            "Accuracy on Training:\n",
            "88.87\n",
            "\n",
            "Accuracy on Testing:\n",
            "80.69\n",
            "\n",
            "Time taken to build the model is 0.58 Seconds\n",
            "\n",
            "Time taken for prediction is 0.16 Seconds\n",
            "\n",
            "................................................\n",
            "Model Logistic Regression with lib linear solver and L2 penalty\n",
            "[[1157  161]\n",
            " [ 280  686]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84      1318\n",
            "           1       0.81      0.71      0.76       966\n",
            "\n",
            "    accuracy                           0.81      2284\n",
            "   macro avg       0.81      0.79      0.80      2284\n",
            "weighted avg       0.81      0.81      0.80      2284\n",
            "\n",
            "\n",
            "Accuracy on Training:\n",
            "88.87\n",
            "\n",
            "Accuracy on Testing:\n",
            "80.69\n",
            "\n",
            "Time taken to build the model is 0.33 Seconds\n",
            "\n",
            "Time taken for prediction is 0.19 Seconds\n",
            "\n",
            "................................................\n",
            "Model Logistic Regression with SAG solver and L2 penalty\n",
            "[[1157  161]\n",
            " [ 280  686]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84      1318\n",
            "           1       0.81      0.71      0.76       966\n",
            "\n",
            "    accuracy                           0.81      2284\n",
            "   macro avg       0.81      0.79      0.80      2284\n",
            "weighted avg       0.81      0.81      0.80      2284\n",
            "\n",
            "\n",
            "Accuracy on Training:\n",
            "88.87\n",
            "\n",
            "Accuracy on Testing:\n",
            "80.69\n",
            "\n",
            "Time taken to build the model is 0.42 Seconds\n",
            "\n",
            "Time taken for prediction is 0.10 Seconds\n",
            "\n",
            "................................................\n",
            "Model Logistic Regression with SAGA solver and L2 penalty\n",
            "[[1157  161]\n",
            " [ 280  686]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.88      0.84      1318\n",
            "           1       0.81      0.71      0.76       966\n",
            "\n",
            "    accuracy                           0.81      2284\n",
            "   macro avg       0.81      0.79      0.80      2284\n",
            "weighted avg       0.81      0.81      0.80      2284\n",
            "\n",
            "\n",
            "Accuracy on Training:\n",
            "88.87\n",
            "\n",
            "Accuracy on Testing:\n",
            "80.69\n",
            "\n",
            "Time taken to build the model is 0.38 Seconds\n",
            "\n",
            "Time taken for prediction is 0.09 Seconds\n",
            "\n",
            "................................................\n",
            "Model Logistic Regression with lib linear solver and L1 penalty\n",
            "[[1125  193]\n",
            " [ 304  662]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82      1318\n",
            "           1       0.77      0.69      0.73       966\n",
            "\n",
            "    accuracy                           0.78      2284\n",
            "   macro avg       0.78      0.77      0.77      2284\n",
            "weighted avg       0.78      0.78      0.78      2284\n",
            "\n",
            "\n",
            "Accuracy on Training:\n",
            "80.73\n",
            "\n",
            "Accuracy on Testing:\n",
            "78.24\n",
            "\n",
            "Time taken to build the model is 0.38 Seconds\n",
            "\n",
            "Time taken for prediction is 0.09 Seconds\n",
            "\n",
            "................................................\n",
            "Model Logistic Regression with SAG solver and L1 penalty\n",
            "[[1125  193]\n",
            " [ 304  662]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.85      0.82      1318\n",
            "           1       0.77      0.69      0.73       966\n",
            "\n",
            "    accuracy                           0.78      2284\n",
            "   macro avg       0.78      0.77      0.77      2284\n",
            "weighted avg       0.78      0.78      0.78      2284\n",
            "\n",
            "\n",
            "Accuracy on Training:\n",
            "80.73\n",
            "\n",
            "Accuracy on Testing:\n",
            "78.24\n",
            "\n",
            "Time taken to build the model is 1.12 Seconds\n",
            "\n",
            "Time taken for prediction is 0.13 Seconds\n"
          ],
          "output_type":"stream"
        },
        {
          "name":"stderr",
          "text":[
            "\/opt\/python\/envs\/default\/lib\/python3.8\/site-packages\/sklearn\/linear_model\/_sag.py:328: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\"The max_iter was reached which means \"\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "\n",
        "from collections import defaultdict\n",
        "# Same 4 models, but with 10-Fold Cross Validation including building and testing time of the model \n",
        "\n",
        "df=pd.read_csv('\/data\/workspace_files\/train.csv')\n",
        "X = df['text']\n",
        "y = df['target']\n",
        "\n",
        "\n",
        "\n",
        "# pipe for each model\n",
        "pipe1 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression()),])\n",
        "\n",
        "pipe2 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression(solver='newton-cg')),])\n",
        "\n",
        "pipe3 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression(solver='liblinear')),])\n",
        "\n",
        "pipe4 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression(solver='sag')),])\n",
        "\n",
        "pipe5 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression(solver='saga')),])\n",
        "\n",
        "pipe6 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression(penalty='l1',solver='liblinear')),])\n",
        "\n",
        "pipe7 = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
        "                     ('clf', LogisticRegression(penalty='l1',solver='saga')),])\n",
        "\n",
        "\n",
        "\n",
        "# list of thr models as pipes\n",
        "list_of_models = {'Logistic Regression with LBFGS solver and L2 penalty':pipe1,\n",
        "                  'Logistic Regression with Newton-CG solver and L2 penalty':pipe2, \n",
        "                  'Logistic Regression with lib linear solver and L2 penalty':pipe3, \n",
        "                  'Logistic Regression with SAG solver and L2 penalty':pipe4, \n",
        "                  'Logistic Regression with SAGA solver and L2 penalty':pipe5, \n",
        "                  'Logistic Regression with lib linear solver and L1 penalty':pipe6, \n",
        "                  'Logistic Regression with SAG solver and L1 penalty':pipe7 }\n",
        "\n",
        "\n",
        "models_accuracy = defaultdict()\n",
        "models_built_time=defaultdict()\n",
        "models_prediction_time=defaultdict()\n",
        "\n",
        "\n",
        "\n",
        "def CV_model_score(models,X,y):\n",
        "    kf = KFold(n_splits=10,random_state=42,shuffle=True)\n",
        "    for model_name, model in list_of_models.items():\n",
        "        single_modele_scores=[]\n",
        "        single_modele_build_time=[]\n",
        "        single_modele_predict_time=[]\n",
        "        single_modele_scores_for_training=[]\n",
        "        for train_index, test_index in kf.split(X,y):\n",
        "         \n",
        "            \n",
        "            start_time =  time.time()%60\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "            text_clf = model.fit(X_train, y_train)\n",
        "            end_time= time.time()%60\n",
        "            if(end_time>start_time):\n",
        "                 time_to_build_the_model=float('%.2f' %(end_time-start_time))\n",
        "            else:\n",
        "                time_to_build_the_model=float('%.2f' %(start_time-end_time))\n",
        "            \n",
        "            single_modele_build_time.append(time_to_build_the_model)\n",
        "            start_time = time.time()%60\n",
        "            predicted = text_clf.predict(X_test)\n",
        "            end_time= time.time()%60\n",
        "            if(end_time>start_time):\n",
        "                time_to_predict=float('%.2f' %(end_time-start_time))\n",
        "            else:\n",
        "                time_to_predict=float('%.2f' %(start_time-end_time))\n",
        "            single_modele_predict_time.append(time_to_predict)\n",
        "            \n",
        "            testing_accuracy=float('%.2f' %(accuracy_score(y_test,predicted)*100))\n",
        "            single_modele_scores.append(testing_accuracy)\n",
        "            \n",
        "            training_accuracy=float('%.2f' %(text_clf.score(X_train,y_train)*100))\n",
        "            single_modele_scores_for_training.append(training_accuracy)\n",
        "        \n",
        "        models_accuracy[model_name]=single_modele_scores\n",
        "        testing_accuracy=statistics.mean(single_modele_scores)\n",
        "        training_accuracy=statistics.mean(single_modele_scores_for_training)\n",
        "        models_built_time=statistics.mean(single_modele_build_time)\n",
        "        models_prediction_time=statistics.mean(single_modele_predict_time)\n",
        "        \n",
        "        print('\\n--------------------------------------\\nModel {}'.format(model_name))\n",
        "        print('Accuracy on Training\\n')\n",
        "        print('%.2f' %training_accuracy)\n",
        "        print('\\nAccuracy on Testing\\n')\n",
        "        print('%.2f' %testing_accuracy)\n",
        "        print('\\nTime taken to build the model is {00:.2f} Seconds'.format(models_built_time))\n",
        "        print('\\nTime taken to predict is {00:.2f} Seconds'.format(models_prediction_time))\n",
        "       \n",
        "\n",
        "\n",
        "\n",
        "CV_model_score(list_of_models,X,y)"
      ],
      "execution_count":0,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "\n",
            "--------------------------------------\n",
            "Model Logistic Regression with LBFGS solver and L2 penalty\n",
            "Accuracy on Training\n",
            "\n",
            "88.72\n",
            "\n",
            "Accuracy on Testing\n",
            "\n",
            "80.26\n",
            "\n",
            "Time taken to build the model is 7.00 Seconds\n",
            "\n",
            "Time taken to predict is 0.10 Seconds\n"
          ],
          "output_type":"stream"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# Method to plot the result\n",
        "\n",
        "def resutls_visulization(list_of_resutls):\n",
        "   \n",
        "    model_names = list(list_of_resutls.keys())\n",
        "    results = [list_of_resutls[model] for model in model_names]\n",
        "    fig = go.Figure()\n",
        "    for model, result in zip(model_names, results):\n",
        "        fig.add_trace(go.Box(\n",
        "            y=result,\n",
        "            name=model,\n",
        "            boxpoints='all',\n",
        "            jitter=0.8,\n",
        "            whiskerwidth=0.9,\n",
        "            marker_size=5,\n",
        "            line_width=2)\n",
        "        )\n",
        "        \n",
        "    \n",
        "    fig.update_layout(\n",
        "    title='Performance of 4 ML Models Using 10-Fold Cross-Validation',\n",
        "    paper_bgcolor='rgb(243, 243, 243)',\n",
        "    plot_bgcolor='rgb(243, 243, 243)',\n",
        "    showlegend=True)\n",
        "    fig.update_yaxes(title_text=\"<b>Accuracy % <\/b>\")\n",
        "    fig.update_xaxes(title_text=\"<b>ML Model<\/b>\")\n",
        "    fig.show()"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "resutls_visulization(models_accuracy)"
      ],
      "execution_count":0,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "Unsupported"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}